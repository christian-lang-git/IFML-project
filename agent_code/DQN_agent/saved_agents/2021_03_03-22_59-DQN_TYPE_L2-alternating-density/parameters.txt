crate_density_list = [0.0, 0.3, 0.75]

CRATE_DISTANCE_DISCOUNT_FACTOR = 0.9
COIN_DISTANCE_DISCOUNT_FACTOR = 0.9

EXPLOIT_SYMMETRY = False
REPLAY_BUFFER_CAPACITY = 250000
BATCH_SIZE = 32
EPSILON_START = 1
EPSILON_END = 0.05
LEARNING_RATE = 0.00025
GAMMA = 0.99
TARGET_STEPS = 10000
#PLANNED_NUMBER_OF_EPOCHS = 20
EPOCH_START_EPSILON_DECAY = 0
EPOCH_STOP_EPSILON_DECAY = 10
EPOCH_LENGTH_TRAINING = 250
EPOCH_LENGTH_VALIDATION = 25
#PLANNED_NUMBER_OF_EPOCHS = 25
#EPOCH_LENGTH_TRAINING = 100
#EPOCH_LENGTH_VALIDATION = 100
#LOOP_NUM_CHECKS = 3#old loop test
LOOP_THRESHOLD = 3#if the agent uses the same action at the same position LOOP_THRESHOLD times, a loop is detected

GAME_REWARDS = {
    e.COIN_COLLECTED: 1,
    #e.KILLED_OPPONENT: 5,
    #e.CRATE_DESTROYED: 0.01,    #for now just a small reward that should encourage the agent to get out of the starting area and discover coins
    e.GOT_KILLED: -1,          #strong penalty for death (this includes suicide)
    #e.KILLED_SELF: -5,          #additional penalty for suicide
    e.INVALID_ACTION: -0.05,    #invalid actions are bad, but should probably not be discouraged too much
    #E_MOVED_TOWARDS_COIN: 0.1,
    #E_MOVED_AWAY_FROM_COIN: -0.11,
    #e.BOMB_DROPPED: -0.2,
    e.WAITED: -0.05,
    E_DANGER_EXCEEDED: -5,
    E_DROPPED_BOMB_BAD: -0.2
}
GAME_REWARD_FACTORS = {
    E_MOVED_TOWARDS_COIN: 2,
    E_MOVED_AWAY_FROM_COIN: 2.2,
    E_MOVED_TOWARDS_CRATE: 1,
    E_MOVED_AWAY_FROM_CRATE: 1.1,
    E_DROPPED_BOMB_NEAR_CRATE: 1, #this is multiplied with number of crates in bomb range / 10
}

MODEL_ARCHITECTURE_DQN_TYPE_L2 = {
    "model_type": DQN_TYPE_L2,
    "process_type": PROCESS_LINEAR,
    "dim_input": [21],
    "dim_output": NUM_ACTIONS,
    "dim_layer_full_1": 128,
    "dim_layer_full_2": 128,
}

